{"name":"Write Dataframe to Postgres","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Requires Postgres JDBC Jar to be installed in Spark instance\n\nurl = \"jdbc:postgresql://192.168.66.4:5432/postgres\"\nproperties = {\"user\": \"postgres\", \"password\": \"secret\", \"driver\": \"org.postgresql.Driver\"}\n\ndf.write.jdbc(url=url, table=\"table_name\", mode=\"overwrite\", properties=properties)"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206073475},"updatedAt":{"$$date":1601206248871},"_id":"27LypAa1GSUOfpUp","folder":null,"tagsPopulated":[]}
{"name":"Stack/Melt/Depivot Dataframe","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"pivot = df.selectExpr(\"product_id\", \"stack(2,'size',size,'colour',colour) AS (feature, value)\")"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206728032},"updatedAt":{"$$date":1601206756954},"_id":"2jZZ6gD0FYSOewwP","folder":null,"tagsPopulated":[]}
{"name":"Groupby Aggregations","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Year wise summary of a selected portion of the dataset\nagg_df = (\n    df.groupBy(\"year\")\n    .agg(\n        min(\"budget\").alias(\"min_budget\"),\n        max(\"budget\").alias(\"max_budget\"),\n        sum(\"revenue\").alias(\"total_revenue\"),\n        avg(\"revenue\").alias(\"avg_revenue\"),\n        mean(\"revenue\").alias(\"mean_revenue\"),\n    )\n    .sort(col(\"year\").desc())\n    .show()\n)\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601249797676},"updatedAt":{"$$date":1601251917248},"_id":"CBFuKw0BJXTZLfTm","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Dual Axis Chart","folderId":"H9_295TO-","content":[{"label":"Fragment 1","language":"python","value":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(x=df.index, y=df[\"total\"], name=\"Spend\"), secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=df.index, y=df[\"orders\"], name=\"Orders\"), secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(title_text=\"Spend and Orders - Monthly\")\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Month\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"<b>Spend</b> Dollars\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>Orders</b> Count\", secondary_y=True)\n\nfig.show()\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601117307194},"updatedAt":{"$$date":1601117435222},"_id":"EFRLMMkDsOtzGZJL","folder":{"id":"H9_295TO-","name":"Plotly","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Import Spark","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"import os\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'\n\nspark = (SparkSession.builder.master('local[4]')\n         .config(\"spark.sql.shuffle.partitions\", 300)\n         .config(\"spark.driver.memory\", \"4g\")\n         .config(\"spark.executor.instances\", \"4\")\n         .config(\"spark.executor.memory\", \"7g\")\n         .appName('Data').getOrCreate())\n\nspark\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601168974380},"updatedAt":{"$$date":1601169045105},"_id":"H664h1QGj5ZkQ6KG","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Filename as Column","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"filename\", input_file_name())"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205508690},"updatedAt":{"$$date":1601205532834},"_id":"HnnltSCRuP4OyZ48","folder":null,"tagsPopulated":[]}
{"name":"Spend Decile","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"from pyspark.ml.feature import QuantileDiscretizer\n\n# Calculate Spend Decile\n# Requires a `transaction` dataframe with `customer_id` and `price_paid` fields\n\nspend_agg = transaction.groupBy(\"customer_id\").agg(sum(\"price_paid\").alias(\"total_spend\"))\n\nspend_decile = (\n    QuantileDiscretizer(numBuckets=10, inputCol=\"total_spend\", outputCol=\"spend_decile\")\n    .fit(spend_agg)\n    .transform(spend_agg)\n)\n\nspend_decile = (\n    spend_decile.where(col(\"spend_decile\").isNotNull())\n    .withColumn(\"spend_decile\", (col(\"spend_decile\") + lit(1)).astype(\"int\"))\n    .select(\"customer_id\", \"spend_decile\")\n)\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601169190228},"updatedAt":{"$$date":1601209465547},"_id":"OfUZTJSapVBdlcuC","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"SHA25 Colum","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"hashed_data\", sha2(df[\"string_data\"]), 256))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205472740},"updatedAt":{"$$date":1601205677527},"_id":"QeCjBscw3w28onbO","folder":null,"tagsPopulated":[]}
{"name":"Pivot Dataframe","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = pivot.groupBy([\"product_id\"]).pivot(\"feature\").agg(first(\"value\"))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206854779},"updatedAt":{"$$date":1601206892599},"_id":"hMZSNTzSKZ0DOBeI","folder":null,"tagsPopulated":[]}
{"name":"String to Date","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Detailed documentation on string date parsing: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\ndf = df.withColumn('date', F.to_timestamp(df['date_string'], 'yyyy-MM-dd'))\n\n \n\n"}],"tags":["eZ6QlkNCuZEzYKhz","s4fMl3y9crq8qlGd"],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1600660454159},"updatedAt":{"$$date":1601205723505},"_id":"nLu6gCLcYYpJbgNx","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[{"name":"spark","_id":"eZ6QlkNCuZEzYKhz","text":"spark"},{"name":"date","_id":"s4fMl3y9crq8qlGd","text":"date"}]}
{"name":"Filter by Null","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Find all the rows for which budget information is not available\ndf.where(df.budget.isNull()).show()\n\n# Similarly, find all the rows for which budget information is available\ndf.where(df.budget.isNotNull()).show()\n\n# N.B. where() is an alias for filter()"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601249857965},"updatedAt":{"$$date":1601251880848},"_id":"oJbFYNFi9mZjyT0R","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Date to String","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Detailed documentation on string date parsing: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\ndf = df.withColumn('date_string', F.date_format(df['date'], 'yyyy-MM-dd'))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601118796029},"updatedAt":{"$$date":1601205988120},"_id":"oRpJtghMT22zl9Gc","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Regular Expression Extraction","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"extension\", regexp_extract(\"filename\", \"\\.[0-9a-z]+$\", 0))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206284589},"updatedAt":{"$$date":1601206324570},"_id":"odzJSqkwcfWODzB0","folder":null,"tagsPopulated":[]}
{"name":"Rank Customer Orders","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"from pyspark.sql.window import Window\ndf =  df.withColumn(\"rank\", F.dense_rank().over(Window.partitionBy(\"customer_id\").orderBy(F.desc(\"order_id\"))))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205560905},"updatedAt":{"$$date":1601205690938},"_id":"u7UoPxsHPZ2TxBOw","folder":null,"tagsPopulated":[]}
