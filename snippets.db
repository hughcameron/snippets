{"name":"Write Dataframe to Postgres","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Requires Postgres JDBC Jar to be installed in Spark instance\n\nurl = \"jdbc:postgresql:\/\/192.168.66.4:5432\/postgres\"\nproperties = {\"user\": \"postgres\", \"password\": \"secret\", \"driver\": \"org.postgresql.Driver\"}\n\ndf.write.jdbc(url=url, table=\"table_name\", mode=\"overwrite\", properties=properties)"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206073475},"updatedAt":{"$$date":1601206248871},"_id":"27LypAa1GSUOfpUp","folder":null,"tagsPopulated":[]}
{"name":"Stack\/Melt\/Depivot Dataframe","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"pivot = df.selectExpr(\"product_id\", \"stack(2,'size',size,'colour',colour) AS (feature, value)\")"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206728032},"updatedAt":{"$$date":1601206756954},"_id":"2jZZ6gD0FYSOewwP","folder":null,"tagsPopulated":[]}
{"name":"Select SQL Expression","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Given a DataFrame\n\ndf = spark.createDataFrame(\n    [(10, 80, \"Alice\"), (5, 50, \"Bob\"), (50, 50, \"Tom\"),], (\"age\", \"height\", \"name\")\n)\n\ndf.show()\n\n# +---+------+-----+\n# |age|height| name|\n# +---+------+-----+\n# | 10|    80|Alice|\n# |  5|    50|  Bob|\n# | 50|    50|  Tom|\n# +---+------+-----+\n\n# Provide any number of expressions to create new columns\n\ndf.selectExpr(\"age * 2\", \"abs(age)\").show()\n\n# +---------+--------+\n# |(age * 2)|abs(age)|\n# +---------+--------+\n# |       20|      10|\n# |       10|       5|\n# |      100|      50|\n# +---------+--------+\n\n# Use select * to include DataFrame\n\ndf.selectExpr(\"*\", \"age * 2\", \"abs(age)\").show()\n\n# +---+------+-----+---------+--------+\n# |age|height| name|(age * 2)|abs(age)|\n# +---+------+-----+---------+--------+\n# | 10|    80|Alice|       20|      10|\n# |  5|    50|  Bob|       10|       5|\n# | 50|    50|  Tom|      100|      50|\n# +---+------+-----+---------+--------+"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601335498466},"updatedAt":{"$$date":1601335888311},"_id":"4PGExhb2hI00WEIj","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Dropping Duplicates by Subset","folderId":null,"content":[{"label":"Fragment 1","language":"python","value":"from pyspark.sql.window import Window\nfrom datetime import date\nfrom pyspark.sql.functions import *\n\nrdd = spark.sparkContext.parallelize(\n    [\n        [1, date(2016, 1, 7), 13.90],\n        [1, date(2016, 1, 7), 10.0],  # duplicate row\n        [1, date(2016, 1, 16), 14.50],\n        [2, date(2016, 1, 9), 10.50],\n        [2, date(2016, 1, 28), 5.50],\n        [3, date(2016, 1, 5), 1.50],\n    ]\n)\n\ndf = rdd.toDF([\"id\", \"date\", \"price\"])\ndf.show()\n\n# +---+----------+-----+\n# | id|      date|price|\n# +---+----------+-----+\n# |  1|2016-01-07| 13.9|\n# |  1|2016-01-07| 10.0|\n# |  1|2016-01-16| 14.5|\n# |  2|2016-01-09| 10.5|\n# |  2|2016-01-28|  5.5|\n# |  3|2016-01-05|  1.5|\n# +---+----------+-----+\n\n\n# Drop duplicates by row_number\ndf.withColumn(\n    \"row_number\", row_number().over(Window.partitionBy(df.id).orderBy(df.date))\n).filter(col(\"row_number\") == 1).show()\n\n# +---+----------+-----+----------+\n# | id|      date|price|row_number|\n# +---+----------+-----+----------+\n# |  3|2016-01-05|  1.5|         1|\n# |  1|2016-01-07| 13.9|         1|\n# |  2|2016-01-09| 10.5|         1|\n# +---+----------+-----+----------+"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1602759715074},"updatedAt":{"$$date":1602759770585},"_id":"AIixv7TbSwCllX76","folder":null,"tagsPopulated":[]}
{"name":"Groupby Aggregations","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Year wise summary of a selected portion of the dataset\nagg_df = (\n    df.groupBy(\"year\")\n    .agg(\n        min(\"budget\").alias(\"min_budget\"),\n        max(\"budget\").alias(\"max_budget\"),\n        sum(\"revenue\").alias(\"total_revenue\"),\n        avg(\"revenue\").alias(\"avg_revenue\"),\n        mean(\"revenue\").alias(\"mean_revenue\"),\n    )\n    .sort(col(\"year\").desc())\n    .show()\n)\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601249797676},"updatedAt":{"$$date":1601251917248},"_id":"CBFuKw0BJXTZLfTm","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Dual Axis Chart","folderId":"H9_295TO-","content":[{"label":"Fragment 1","language":"python","value":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(x=df.index, y=df[\"total\"], name=\"Spend\"), secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=df.index, y=df[\"orders\"], name=\"Orders\"), secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(title_text=\"Spend and Orders - Monthly\")\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Month\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"<b>Spend<\/b> Dollars\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>Orders<\/b> Count\", secondary_y=True)\n\nfig.show()\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601117307194},"updatedAt":{"$$date":1601117435222},"_id":"EFRLMMkDsOtzGZJL","folder":{"id":"H9_295TO-","name":"Plotly","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Import Spark","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"import os\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'\n\nspark = (SparkSession.builder.master('local[4]')\n         .config(\"spark.sql.shuffle.partitions\", 300)\n         .config(\"spark.driver.memory\", \"4g\")\n         .config(\"spark.executor.instances\", \"4\")\n         .config(\"spark.executor.memory\", \"7g\")\n         .appName('Data').getOrCreate())\n\nspark\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601168974380},"updatedAt":{"$$date":1601169045105},"_id":"H664h1QGj5ZkQ6KG","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Filename as Column","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"filename\", input_file_name())"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205508690},"updatedAt":{"$$date":1601205532834},"_id":"HnnltSCRuP4OyZ48","folder":null,"tagsPopulated":[]}
{"name":"Generate UUID","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Given a DataFrame\n\ndf = spark.createDataFrame(\n    [(10, 80, \"Alice\"), (5, 50, \"Bob\"), (50, 50, \"Tom\"),], (\"age\", \"height\", \"name\")\n)\n\ndf.show()\n\n# +---+------+-----+\n# |age|height| name|\n# +---+------+-----+\n# | 10|    80|Alice|\n# |  5|    50|  Bob|\n# | 50|    50|  Tom|\n# +---+------+-----+\n\ndf.selectExpr(\"*\", \"uuid() AS id\").show()\n\n# +---+------+-----+--------------------+\n# |age|height| name|                  id|\n# +---+------+-----+--------------------+\n# | 10|    80|Alice|a7157b14-813d-444...|\n# |  5|    50|  Bob|08d1ba1d-1b7b-4a6...|\n# | 50|    50|  Tom|ec0ab7c1-5de8-430...|\n# +---+------+-----+--------------------+"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601335846438},"updatedAt":{"$$date":1601335886004},"_id":"JJGg4XzE8fu6GrFP","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Backward, Forward Fill","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"from pyspark.sql.window import Window\n\n# Given a DataFrame structured as:\n\ndf = spark.createDataFrame(\n    [\n        (\"d1\", None, \"New York\"),\n        (\"d2\", 10, \"New York\"),\n        (\"d3\", None, \"New York\"),\n        (\"d4\", 30, \"New York\"),\n        (\"d5\", None, \"New York\"),\n        (\"d6\", None, \"New York\"),\n        (\"d1\", None, \"London\"),\n        (\"d2\", 8, \"London\"),\n        (\"d3\", None, \"London\"),\n        (\"d4\", 16, \"London\"),\n        (\"d5\", None, \"London\"),\n        (\"d6\", None, \"London\"),\n    ],\n    (\"day\", \"temperature\", \"city\"),\n)\n\n# +---+-----------+--------+\n# |day|temperature|    city|\n# +---+-----------+--------+\n# | d1|       null|New York|\n# | d2|         10|New York|\n# | d3|       null|New York|\n# | d4|         30|New York|\n# | d5|       null|New York|\n# | d6|       null|New York|\n# | d1|       null|  London|\n# | d2|          8|  London|\n# | d3|       null|  London|\n# | d4|         16|  London|\n# | d5|       null|  London|\n# | d6|       null|  London|\n# +---+-----------+--------+\n\n# Fill Forward by setting a window from first to current row with respect to orderBy variable\nforward_window = (\n    Window.partitionBy(\"city\")\n    .orderBy(\"day\")\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n)\ndf = df.withColumn(\n    \"fill_forward\", last(\"temperature\", ignorenulls=True).over(forward_window)\n)\n\n# Fill backward by setting a window from current row to last with respect to orderBy variable\nbackward_window = (\n    Window.partitionBy(\"city\")\n    .orderBy(\"day\")\n    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n)\ndf = df.withColumn(\n    \"fill_backward\", first(\"temperature\", ignorenulls=True).over(backward_window)\n)\n\n# N.B. Ranges of fill are bounded by the variable used as the `partitionBy` argument\n# To fill values across the DataFrame use `partionBy` with no argument\n\ndf.show()\n\n# +---+-----------+--------+------------+-------------+\n# |day|temperature|    city|fill_forward|fill_backward|\n# +---+-----------+--------+------------+-------------+\n# | d1|       null|  London|        null|            8|\n# | d2|          8|  London|           8|            8|\n# | d3|       null|  London|           8|           16|\n# | d4|         16|  London|          16|           16|\n# | d5|       null|  London|          16|         null|\n# | d6|       null|  London|          16|         null|\n# | d1|       null|New York|        null|           10|\n# | d2|         10|New York|          10|           10|\n# | d3|       null|New York|          10|           30|\n# | d4|         30|New York|          30|           30|\n# | d5|       null|New York|          30|         null|\n# | d6|       null|New York|          30|         null|\n# +---+-----------+--------+------------+-------------+"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601327384310},"updatedAt":{"$$date":1601336828808},"_id":"JMxDDonThCq11OZX","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Spend Decile","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"from pyspark.ml.feature import QuantileDiscretizer\n\n# Calculate Spend Decile\n# Requires a `transaction` dataframe with `customer_id` and `price_paid` fields\n\nspend_agg = transaction.groupBy(\"customer_id\").agg(sum(\"price_paid\").alias(\"total_spend\"))\n\nspend_decile = (\n    QuantileDiscretizer(numBuckets=10, inputCol=\"total_spend\", outputCol=\"spend_decile\")\n    .fit(spend_agg)\n    .transform(spend_agg)\n)\n\nspend_decile = (\n    spend_decile.where(col(\"spend_decile\").isNotNull())\n    .withColumn(\"spend_decile\", (col(\"spend_decile\") + lit(1)).astype(\"int\"))\n    .select(\"customer_id\", \"spend_decile\")\n)\n"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601169190228},"updatedAt":{"$$date":1601209465547},"_id":"OfUZTJSapVBdlcuC","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"SHA25 Colum","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"hashed_data\", sha2(df[\"string_data\"]), 256))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205472740},"updatedAt":{"$$date":1601205677527},"_id":"QeCjBscw3w28onbO","folder":null,"tagsPopulated":[]}
{"name":"String Arrary to Spark Array","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Given the below DataFrame with an array stored as a string, we can access the array data\n\ndf = spark.createDataFrame(\n    [\n        (\"north\", \"['e', 'c']\"),\n        (\"east\",  \"['a', 'b']\"),\n        (\"south\", \"['t', 'f']\"),\n        (\"west\",  \"['g', 'h']\"),\n    ],\n    (\"sector\", \"observations\"),\n)\n\ndf.show()\n\n# +------+------------+\n# |sector|observations|\n# +------+------------+\n# | north|  ['e', 'c']|\n# |  east|  ['a', 'b']|\n# | south|  ['t', 'f']|\n# |  west|  ['g', 'h']|\n# +------+------------+\n\n# Set a Spark Schema matching the data types of the array and parse as JSON using `from_json`\n# A list of available types is documented here: https:\/\/spark.apache.org\/docs\/latest\/sql-ref-datatypes.html\n\ndf = df.withColumn(\"observations\", from_json(\"observations\", ArrayType(StringType())))\n\n\ndf = df.withColumn(\"first\", col(\"observations\").getItem(0))\n\ndf.show()\n\n# +------+------------+-----+\n# |sector|observations|first|\n# +------+------------+-----+\n# | north|      [e, c]|    e|\n# |  east|      [a, b]|    a|\n# | south|      [t, f]|    t|\n# |  west|      [g, h]|    g|\n# +------+------------+-----+"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601419974647},"updatedAt":{"$$date":1601420661238},"_id":"ZqmFHnpk8glUKzPa","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Pivot Dataframe","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = pivot.groupBy([\"product_id\"]).pivot(\"feature\").agg(first(\"value\"))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206854779},"updatedAt":{"$$date":1601206892599},"_id":"hMZSNTzSKZ0DOBeI","folder":null,"tagsPopulated":[]}
{"name":"String to Date","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Detailed documentation on string date parsing: https:\/\/spark.apache.org\/docs\/latest\/sql-ref-datetime-pattern.html\n\ndf = df.withColumn('date', F.to_timestamp(df['date_string'], 'yyyy-MM-dd'))\n\n \n\n"}],"tags":["eZ6QlkNCuZEzYKhz","s4fMl3y9crq8qlGd"],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1600660454159},"updatedAt":{"$$date":1601205723505},"_id":"nLu6gCLcYYpJbgNx","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[{"name":"spark","_id":"eZ6QlkNCuZEzYKhz","text":"spark"},{"name":"date","_id":"s4fMl3y9crq8qlGd","text":"date"}]}
{"name":"Filter by Null","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Find all the rows for which budget information is not available\ndf.where(df.budget.isNull()).show()\n\n# Similarly, find all the rows for which budget information is available\ndf.where(df.budget.isNotNull()).show()\n\n# N.B. where() is an alias for filter()"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601249857965},"updatedAt":{"$$date":1601251880848},"_id":"oJbFYNFi9mZjyT0R","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"python"},"tagsPopulated":[]}
{"name":"Date to String","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"# Detailed documentation on string date parsing: https:\/\/spark.apache.org\/docs\/latest\/sql-ref-datetime-pattern.html\n\ndf = df.withColumn('date_string', F.date_format(df['date'], 'yyyy-MM-dd'))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601118796029},"updatedAt":{"$$date":1601205988120},"_id":"oRpJtghMT22zl9Gc","folder":{"id":"n1tE3NbTo","name":"Spark","open":false,"defaultLanguage":"text"},"tagsPopulated":[]}
{"name":"Regular Expression Extraction","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"df = df.withColumn(\"extension\", regexp_extract(\"filename\", \"\\.[0-9a-z]+$\", 0))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601206284589},"updatedAt":{"$$date":1601206324570},"_id":"odzJSqkwcfWODzB0","folder":null,"tagsPopulated":[]}
{"name":"Rank Customer Orders","folderId":"n1tE3NbTo","content":[{"label":"Fragment 1","language":"python","value":"from pyspark.sql.window import Window\ndf =  df.withColumn(\"rank\", F.dense_rank().over(Window.partitionBy(\"customer_id\").orderBy(F.desc(\"order_id\"))))"}],"tags":[],"isFavorites":false,"isDeleted":false,"createdAt":{"$$date":1601205560905},"updatedAt":{"$$date":1601205690938},"_id":"u7UoPxsHPZ2TxBOw","folder":null,"tagsPopulated":[]}